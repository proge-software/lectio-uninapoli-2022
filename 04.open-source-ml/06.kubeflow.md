# Kubeflow <!-- omit in toc -->

## Contents <!-- omit in toc -->

- [Kubeflow](#kubeflow)
  - [Architecture](#architecture)
  - [ML Workflow](#ml-workflow)
  - [Kubeflow components in the ML workflow](#kubeflow-components-in-the-ml-workflow)
- [Kubeflow Pipelines](#kubeflow-pipelines)
  - [What is a Pipeline?](#what-is-a-pipeline)
- [Demo](#demo)
  - [Prerequisites](#prerequisites)
  - [Demo](#demo-1)
- [References](#references)
- [More Links](#more-links)

## Kubeflow

The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable.
Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures.
Anywhere you are running Kubernetes, you should be able to run Kubeflow.

### Architecture

The following diagram shows Kubeflow as a platform for arranging the components of your ML system on top of Kubernetes:

![image](https://www.kubeflow.org/docs/images/kubeflow-overview-platform-diagram.svg)

Kubeflow builds on Kubernetes as a system for deploying, scaling, and managing complex systems.

### ML Workflow

When you develop and deploy an ML system, the ML workflow typically consists of several stages.
Developing an ML system is an iterative process.
You need to evaluate the output of various stages of the ML workflow, and apply changes to the model and parameters when necessary to ensure the model keeps producing the results you need.

For the sake of simplicity, the following diagram shows the workflow stages in sequence.
The arrow at the end of the workflow points back into the flow to indicate the iterative nature of the process:

![image](https://www.kubeflow.org/docs/images/kubeflow-overview-workflow-diagram-1.svg)

In the experimental phase, you develop your model based on initial assumptions, and test and update the model iteratively to produce the results you’re looking for:

- Identify the problem you want the ML system to solve.
- Collect and analyze the data you need to train your ML model.
- Choose an ML framework and algorithm, and code the initial version of your model.
- Experiment with the data and with training your model.
- Tune the model hyperparameters to ensure the most efficient processing and the most accurate results possible.

In the production phase, you deploy a system that performs the following processes:

- Transform the data into the format that your training system needs. To ensure that your model behaves consistently during training and prediction, the transformation process must be the same in the experimental and production phases.
- Train the ML model.
- Serve the model for online prediction or for running in batch mode.
- Monitor the model’s performance, and feed the results into your processes for tuning or retraining the model.

### Kubeflow components in the ML workflow

The next diagram adds Kubeflow to the workflow, showing which Kubeflow components are useful at each stage:

![image](https://www.kubeflow.org/docs/images/kubeflow-overview-workflow-diagram-2.svg)

## Kubeflow Pipelines

Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.

The Kubeflow Pipelines platform consists of:

- A user interface (UI) for managing and tracking experiments, jobs, and runs.
- An engine for scheduling multi-step ML workflows.
- An SDK for defining and manipulating pipelines and components.
- Notebooks for interacting with the system using the SDK.

The following are the goals of Kubeflow Pipelines:

- End-to-end orchestration: enabling and simplifying the orchestration of machine learning pipelines.
- Easy experimentation: making it easy for you to try numerous ideas and techniques and manage your various trials/experiments.
- Easy re-use: enabling you to re-use components and pipelines to quickly create end-to-end solutions without having to rebuild each time.

### What is a Pipeline?

A pipeline is a description of an ML workflow, including all of the components in the workflow and how they combine in the form of a graph.
The pipeline includes the definition of the inputs (parameters) required to run the pipeline and the inputs and outputs of each component.

After developing your pipeline, you can upload and share it on the Kubeflow Pipelines UI.

A pipeline component is a self-contained set of user code, packaged as a Docker image, that performs one step in the pipeline.
For example, a component can be responsible for data preprocessing, data transformation, model training, and so on.

### What is Kubeflow Notebook?

Kubeflow Notebooks provides a way to run web-based development environments inside your Kubernetes cluster by running them inside Pods.

Some key features include:
- Native support for JupyterLab, RStudio, and Visual Studio Code (code-server).
- Users can create notebook containers directly in the cluster, rather than locally on their workstations.
- Admins can provide standard notebook images for their organization with required packages pre-installed.
- Access control is managed by Kubeflow’s RBAC, enabling easier notebook sharing across the organization.


### What are Kubeflow Models?
![image](https://www.kubeflow.org/docs/external-add-ons/kserve/pics/kserve.png)

KServe enables serverless inferencing on Kubernetes and provides performant, high abstraction interfaces for common machine learning (ML) frameworks like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX to solve production model serving use cases.

### What are Kubeflow Experiments?
- Auto ML Experiments:
  - Katib is a Kubernetes-native project for automated machine learning (AutoML). Katib supports hyperparameter tuning, early stopping and neural architecture search (NAS). Learn more about AutoML at fast.ai, Google Cloud, Microsoft Azure or Amazon SageMaker.

- KFP Experiments:
  - The Kubeflow Pipelines platform consists of:

    - A user interface (UI) for managing and tracking experiments, jobs, and runs.
    - An engine for scheduling multi-step ML workflows.
    - An SDK for defining and manipulating pipelines and components.
    - Notebooks for interacting with the system using the SDK.

## Demo

### Prerequisites

[Here](src/06.kubeflow/README.md) you can find details on how to setup the infrastructure needed for the following demos, using Terraform and Azure.

### Demo

<!-- TODO -->

## References

- https://www.kubeflow.org/docs/started/introduction/
- https://www.kubeflow.org/docs/started/architecture/
- https://www.kubeflow.org/docs/external-add-ons/kserve/kserve/
- https://www.kubeflow.org/docs/components/katib/overview/
- https://www.kubeflow.org/docs/components/notebooks/overview/
- https://www.kubeflow.org/docs/components/pipelines/introduction/

## More Links

## Agenda <!-- omit in toc -->

1. [Linux](02.linux.md)
2. [Containers](03.containers.md)
3. [Orchestrators](04.orchestrators.md)
4. [Kubernetes](05.kubernetes.md)
5. [**Kubeflow**](06.kubeflow.md)
6. [Q&A](07.q&a.md)
